{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/user/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "__Author__ = 'Zijie Zhao'\n",
    "__Create__ = '04/26/2022'\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from scene_seg.dataset import TrainDataset\n",
    "\n",
    "# create a dataset and dataloader\n",
    "def imresize(im, size, interp='bilinear'):\n",
    "    if interp == 'nearest':\n",
    "        resample = Image.NEAREST\n",
    "    elif interp == 'bilinear':\n",
    "        resample = Image.BILINEAR\n",
    "    elif interp == 'bicubic':\n",
    "        resample = Image.BICUBIC\n",
    "    else:\n",
    "        raise Exception('resample method undefined!')\n",
    "\n",
    "    return im.resize(size, resample)\n",
    "\n",
    "\n",
    "class BaseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, odgt, opt, **kwargs):\n",
    "        # parse options\n",
    "        self.imgSizes = opt['imgSizes']\n",
    "        self.imgMaxSize = opt['imgMaxSize']\n",
    "        # max down sampling rate of network to avoid rounding during conv or pooling\n",
    "        self.padding_constant = opt['padding_constant']\n",
    "\n",
    "        # parse the input list\n",
    "        self.parse_input_list(odgt, **kwargs)\n",
    "\n",
    "        # mean and std\n",
    "        self.normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def parse_input_list(self, odgt, max_sample=-1, start_idx=-1, end_idx=-1):\n",
    "        if isinstance(odgt, list):\n",
    "            self.list_sample = odgt\n",
    "        elif isinstance(odgt, str):\n",
    "            self.list_sample = [json.loads(x.rstrip()) for x in open(odgt, 'r')]\n",
    "\n",
    "        if max_sample > 0:\n",
    "            self.list_sample = self.list_sample[0:max_sample]\n",
    "        if start_idx >= 0 and end_idx >= 0:     # divide file list\n",
    "            self.list_sample = self.list_sample[start_idx:end_idx]\n",
    "\n",
    "        self.num_sample = len(self.list_sample)\n",
    "        assert self.num_sample > 0\n",
    "        print('# samples: {}'.format(self.num_sample))\n",
    "\n",
    "    def img_transform(self, img):\n",
    "        # 0-255 to 0-1\n",
    "        img = np.float32(np.array(img)) / 255.\n",
    "        img = img.transpose((2, 0, 1))\n",
    "        # img = self.normalize(torch.from_numpy(img.copy()))\n",
    "        img = torch.from_numpy(img.copy())\n",
    "        return img\n",
    "\n",
    "    def segm_transform(self, segm):\n",
    "        # to tensor, -1 to 149\n",
    "        segm = torch.from_numpy(np.array(segm)).long() - 1\n",
    "        return segm\n",
    "\n",
    "    # Round x to the nearest multiple of p and x' >= x\n",
    "    def round2nearest_multiple(self, x, p):\n",
    "        return ((x - 1) // p + 1) * p\n",
    "\n",
    "\n",
    "class TrainDataset(BaseDataset):\n",
    "    def __init__(self, root_dataset, odgt, opt, batch_per_gpu=1, **kwargs):\n",
    "        super(TrainDataset, self).__init__(odgt, opt, **kwargs)\n",
    "        self.root_dataset = root_dataset\n",
    "        # down sampling rate of segm labe\n",
    "        self.segm_downsampling_rate = opt['segm_downsampling_rate']\n",
    "        self.batch_per_gpu = batch_per_gpu\n",
    "\n",
    "        # classify images into two classes: 1. h > w and 2. h <= w\n",
    "        self.batch_record_list = [[], []]\n",
    "\n",
    "        # override dataset length when trainig with batch_per_gpu > 1\n",
    "        self.cur_idx = 0\n",
    "        self.if_shuffled = False\n",
    "\n",
    "    def _get_sub_batch(self):\n",
    "        while True:\n",
    "            # get a sample record\n",
    "            this_sample = self.list_sample[self.cur_idx]\n",
    "            if this_sample['height'] > this_sample['width']:\n",
    "                self.batch_record_list[0].append(this_sample) # h > w, go to 1st class\n",
    "            else:\n",
    "                self.batch_record_list[1].append(this_sample) # h <= w, go to 2nd class\n",
    "\n",
    "            # update current sample pointer\n",
    "            self.cur_idx += 1\n",
    "            if self.cur_idx >= self.num_sample:\n",
    "                self.cur_idx = 0\n",
    "                np.random.shuffle(self.list_sample)\n",
    "\n",
    "            if len(self.batch_record_list[0]) == self.batch_per_gpu:\n",
    "                batch_records = self.batch_record_list[0]\n",
    "                self.batch_record_list[0] = []\n",
    "                break\n",
    "            elif len(self.batch_record_list[1]) == self.batch_per_gpu:\n",
    "                batch_records = self.batch_record_list[1]\n",
    "                self.batch_record_list[1] = []\n",
    "                break\n",
    "        return batch_records\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # NOTE: random shuffle for the first time. shuffle in __init__ is useless\n",
    "        if not self.if_shuffled:\n",
    "            np.random.seed(index)\n",
    "            np.random.shuffle(self.list_sample)\n",
    "            self.if_shuffled = True\n",
    "\n",
    "        # get sub-batch candidates\n",
    "        batch_records = self._get_sub_batch()\n",
    "\n",
    "        # resize all images' short edges to the chosen size\n",
    "        if isinstance(self.imgSizes, list) or isinstance(self.imgSizes, tuple):\n",
    "            this_short_size = np.random.choice(self.imgSizes)\n",
    "        else:\n",
    "            this_short_size = self.imgSizes\n",
    "\n",
    "        # calculate the BATCH's height and width\n",
    "        # since we concat more than one samples, the batch's h and w shall be larger than EACH sample\n",
    "        batch_widths = np.zeros(self.batch_per_gpu, np.int32)\n",
    "        batch_heights = np.zeros(self.batch_per_gpu, np.int32)\n",
    "        for i in range(self.batch_per_gpu):\n",
    "            img_height, img_width = batch_records[i]['height'], batch_records[i]['width']\n",
    "            this_scale = min(\n",
    "                this_short_size / min(img_height, img_width), \\\n",
    "                self.imgMaxSize / max(img_height, img_width))\n",
    "            batch_widths[i] = img_width * this_scale\n",
    "            batch_heights[i] = img_height * this_scale\n",
    "\n",
    "        # Here we must pad both input image and segmentation map to size h' and w' so that p | h' and p | w'\n",
    "        batch_width = np.max(batch_widths)\n",
    "        batch_height = np.max(batch_heights)\n",
    "        batch_width = int(self.round2nearest_multiple(batch_width, self.padding_constant))\n",
    "        batch_height = int(self.round2nearest_multiple(batch_height, self.padding_constant))\n",
    "\n",
    "        batch_images = torch.zeros(\n",
    "            self.batch_per_gpu, 3, batch_height, batch_width)\n",
    "        batch_segms = torch.zeros(\n",
    "            self.batch_per_gpu,\n",
    "            batch_height,\n",
    "            batch_width).long()\n",
    "\n",
    "        for i in range(self.batch_per_gpu):\n",
    "            this_record = batch_records[i]\n",
    "\n",
    "            # load image and label\n",
    "            image_path = os.path.join(self.root_dataset, this_record['fpath_img'])\n",
    "            segm_path = os.path.join(self.root_dataset, this_record['fpath_segm'])\n",
    "\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "            segm = Image.open(segm_path)\n",
    "            assert(segm.mode == \"L\")\n",
    "            assert(img.size[0] == segm.size[0])\n",
    "            assert(img.size[1] == segm.size[1])\n",
    "\n",
    "            # random_flip\n",
    "            if np.random.choice([0, 1]):\n",
    "                img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                segm = segm.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "            # note that each sample within a mini batch has different scale param\n",
    "            img = imresize(img, (batch_widths[i], batch_heights[i]), interp='bilinear')\n",
    "            segm = imresize(segm, (batch_widths[i], batch_heights[i]), interp='nearest')\n",
    "\n",
    "            # image transform, to torch float tensor 3xHxW\n",
    "            img = self.img_transform(img)\n",
    "\n",
    "            # segm transform, to torch long tensor HxW\n",
    "            segm = self.segm_transform(segm)\n",
    "\n",
    "            # put into batch arrays\n",
    "            batch_images[i][:, :img.shape[1], :img.shape[2]] = img\n",
    "            batch_segms[i][:segm.shape[0], :segm.shape[1]] = segm\n",
    "\n",
    "        output = dict()\n",
    "        output['img_data'] = batch_images\n",
    "        output['seg_label'] = batch_segms\n",
    "\n",
    "        return output\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(1e10) # It's a fake length due to the trick that every loader maintains its own list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/software/anaconda3/2019.10_powerai/envs/powerai/lib/python3.7/site-packages/torch/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.6 (default, Jan  8 2020, 19:13:59) \n",
      "[GCC 7.3.0] :: Anaconda, Inc. on linux\n",
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      ">>> \n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "KeyboardInterrupt\n",
      ">>> /usr/bin/sh: import: command not found\n"
     ]
    }
   ],
   "source": [
    "!python\n",
    "!import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     /home/software/anaconda3/2019.10_powerai\n",
      "Hackathon_2020_Environment_testing     /home/software/anaconda3/2019.10_powerai/envs/Hackathon_2020_Environment_testing\n",
      "powerai                  /home/software/anaconda3/2019.10_powerai/envs/powerai\n",
      "                         /nobackup/users/zijiezha/anaconda3\n",
      "                      *  /nobackup/users/zijiezha/anaconda3/envs/opence\n",
      "                         /nobackup/users/zijiezha/anaconda3/envs/py36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda info --envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\n",
      "To initialize your shell, run\n",
      "\n",
      "    $ conda init <SHELL_NAME>\n",
      "\n",
      "Currently supported shells are:\n",
      "  - bash\n",
      "  - fish\n",
      "  - tcsh\n",
      "  - xonsh\n",
      "  - zsh\n",
      "  - powershell\n",
      "\n",
      "See 'conda init --help' for more information and options.\n",
      "\n",
      "IMPORTANT: You may need to close and restart your shell after running 'conda init'.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda activate /home/software/anaconda3/2019.10_powerai/envs/powerai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     /home/software/anaconda3/2019.10_powerai\n",
      "Hackathon_2020_Environment_testing     /home/software/anaconda3/2019.10_powerai/envs/Hackathon_2020_Environment_testing\n",
      "powerai                  /home/software/anaconda3/2019.10_powerai/envs/powerai\n",
      "                         /nobackup/users/zijiezha/anaconda3\n",
      "                      *  /nobackup/users/zijiezha/anaconda3/envs/opence\n",
      "                         /nobackup/users/zijiezha/anaconda3/envs/py36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def createDeepLabv3(outputchannels=1):\n",
    "#     \"\"\"DeepLabv3 class with custom head\n",
    "#     Args:\n",
    "#         outputchannels (int, optional): The number of output channels\n",
    "#         in your dataset masks. Defaults to 1.\n",
    "#     Returns:\n",
    "#         model: Returns the DeepLabv3 model with the ResNet101 backbone.\n",
    "#     \"\"\"\n",
    "#     model = models.segmentation.deeplabv3_resnet101(pretrained=True,\n",
    "#                                                     progress=True)\n",
    "#     model.classifier = DeepLabHead(2048, outputchannels)\n",
    "#     model.train()\n",
    "#     return model\n",
    "\n",
    "\n",
    "def createDeepLabv3(outputchannels=1, keep_feature_extract=False, use_pretrained=True):\n",
    "    \"\"\" DeepLabV3 pretrained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n",
    "    \"\"\"\n",
    "    model_deeplabv3 = models.segmentation.deeplabv3_resnet101(pretrained=use_pretrained, progress=True)\n",
    "    model_deeplabv3.aux_classifier = None\n",
    "    if keep_feature_extract:\n",
    "        for param in model_deeplabv3.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    model_deeplabv3.classifier = models.segmentation.deeplabv3.DeepLabHead(2048, outputchannels)\n",
    "\n",
    "    return model_deeplabv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.initialized = False\n",
    "        self.val = None\n",
    "        self.avg = None\n",
    "        self.sum = None\n",
    "        self.count = None\n",
    "\n",
    "    def initialize(self, val, weight):\n",
    "        self.val = val\n",
    "        self.avg = val\n",
    "        self.sum = val * weight\n",
    "        self.count = weight\n",
    "        self.initialized = True\n",
    "\n",
    "    def update(self, val, weight=1):\n",
    "        if not self.initialized:\n",
    "            self.initialize(val, weight)\n",
    "        else:\n",
    "            self.add(val, weight)\n",
    "\n",
    "    def add(self, val, weight):\n",
    "        self.val = val\n",
    "        self.sum += val * weight\n",
    "        self.count += weight\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def value(self):\n",
    "        return self.val\n",
    "\n",
    "    def average(self):\n",
    "        return self.avg\n",
    "\n",
    "def pixel_acc(pred, label):\n",
    "        _, preds = torch.max(pred, dim=1)\n",
    "        valid = (label >= 0).long()\n",
    "        acc_sum = torch.sum(valid * (preds == label).long())\n",
    "        pixel_sum = torch.sum(valid)\n",
    "        acc = acc_sum.float() / (pixel_sum.float() + 1e-10)\n",
    "        return acc\n",
    "\n",
    "def train(segmentation_module, iterator, optimizers, criterion, history, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    ave_total_loss = AverageMeter()\n",
    "    ave_acc = AverageMeter()\n",
    "\n",
    "    segmentation_module.train()\n",
    "\n",
    "    # main loop\n",
    "    tic = time.time()\n",
    "    for i in range(5000):\n",
    "        # load a batch of data\n",
    "        batch_data = next(iterator)[0]\n",
    "        batch_data['img_data'] = batch_data['img_data'].cuda()\n",
    "        batch_data['seg_label'] = batch_data['seg_label'].cuda()\n",
    "        data_time.update(time.time() - tic)\n",
    "        segmentation_module.zero_grad()\n",
    "\n",
    "        # adjust learning rate\n",
    "        cur_iter = i + (epoch - 1) * 5000\n",
    "\n",
    "        # # forward pass\n",
    "        pred = segmentation_module(batch_data['img_data'])['out']\n",
    "        loss = criterion(pred, batch_data['seg_label'])\n",
    "        acc = pixel_acc(pred, batch_data['seg_label'])\n",
    "\n",
    "        loss = loss.mean()\n",
    "        acc = acc.mean()\n",
    "\n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizers.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - tic)\n",
    "        tic = time.time()\n",
    "\n",
    "        # update average loss and acc\n",
    "        ave_total_loss.update(loss.data.item())\n",
    "        ave_acc.update(acc.data.item()*100)\n",
    "\n",
    "        # calculate accuracy, and display\n",
    "        if i % 20 == 0:\n",
    "            print('Epoch: [{}][{}/{}], Avg Train Time: {:.2f}, Avg Data time: {:.2f}, '\n",
    "                  'Accuracy: {:4.2f}, Loss: {:.6f}'\n",
    "                  .format(epoch, i, 5000,\n",
    "                          batch_time.average(), data_time.average(),\n",
    "                          ave_acc.average(), ave_total_loss.average()))\n",
    "\n",
    "            fractional_epoch = epoch - 1 + 1. * i / 5000\n",
    "            history['train']['epoch'].append(fractional_epoch)\n",
    "            history['train']['loss'].append(loss.data.item())\n",
    "            history['train']['acc'].append(acc.data.item())\n",
    "          \n",
    "            writer.add_scalar('training loss',\n",
    "                ave_total_loss.average(),\n",
    "                cur_iter)\n",
    "            writer.add_scalar('training accuracy',\n",
    "                ave_acc.average(),\n",
    "                cur_iter)\n",
    "\n",
    "def checkpoint(model, history, epoch):\n",
    "    print('Saving checkpoints...')\n",
    "\n",
    "    dict_model = model.state_dict()\n",
    "\n",
    "    torch.save(\n",
    "        history,\n",
    "        '{}/history_epoch_{}.pth'.format(DIR, epoch))\n",
    "    torch.save(\n",
    "        dict_model,\n",
    "        '{}/model_epoch_{}.pth'.format(DIR, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class UserScatteredDataParallel(DictGatherDataParallel):\n",
    "#     def scatter(self, inputs, kwargs, device_ids):\n",
    "#         assert len(inputs) == 1\n",
    "#         inputs = inputs[0]\n",
    "#         inputs = _async_copy_stream(inputs, device_ids)\n",
    "#         inputs = [[i] for i in inputs]\n",
    "#         assert len(kwargs) == 0\n",
    "#         kwargs = [{} for _ in range(len(inputs))]\n",
    "\n",
    "#         return inputs, kwargs\n",
    "\n",
    "\n",
    "\n",
    "def user_scattered_collate(batch):\n",
    "        return batch\n",
    "\n",
    "# def patch_replication_callback(data_parallel):\n",
    "#     \"\"\"\n",
    "#     Monkey-patch an existing `DataParallel` object. Add the replication callback.\n",
    "#     Useful when you have customized `DataParallel` implementation.\n",
    "\n",
    "#     Examples:\n",
    "#         > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n",
    "#         > sync_bn = DataParallel(sync_bn, device_ids=[0, 1])\n",
    "#         > patch_replication_callback(sync_bn)\n",
    "#         # this is equivalent to\n",
    "#         > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n",
    "#         > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n",
    "#     \"\"\"\n",
    "\n",
    "#     assert isinstance(data_parallel, DataParallel)\n",
    "\n",
    "#     old_replicate = data_parallel.replicate\n",
    "\n",
    "#     @functools.wraps(old_replicate)\n",
    "#     def new_replicate(module, device_ids):\n",
    "#         modules = old_replicate(module, device_ids)\n",
    "#         execute_replication_callbacks(modules)\n",
    "#         return modules\n",
    "\n",
    "#     data_parallel.replicate = new_replicate\n",
    "\n",
    "# Dataset and Loader\n",
    "def main():\n",
    "    gpus = [0]\n",
    "    DATASET = {'root_dataset': \"./data/\", \n",
    "              'list_train': \"./data/training.odgt\",\n",
    "              'list_val': \"./data/validation.odgt\", \n",
    "              'num_class': 150, \n",
    "              'imgSizes': (300, 375, 450, 525, 600), \n",
    "              'imgMaxSize': 1000, \n",
    "              'padding_constant': 8, \n",
    "              'segm_downsampling_rate': 8, \n",
    "              'random_flip': True}\n",
    "    dataset_train = TrainDataset(\n",
    "            \"./data/\",\n",
    "            \"./data/training.odgt\",\n",
    "            DATASET,\n",
    "            batch_per_gpu=2)\n",
    "    loader_train = torch.utils.data.DataLoader(\n",
    "            dataset_train,\n",
    "            batch_size=len(gpus),  # we have modified data_parallel\n",
    "            shuffle=False,  # we do not use this param\n",
    "            collate_fn=user_scattered_collate,\n",
    "            num_workers=2,\n",
    "            drop_last=True,\n",
    "            pin_memory=True)\n",
    "    print('1 Epoch = {} iters'.format(5000))\n",
    "\n",
    "    # create loader iterator\n",
    "    iterator_train = iter(loader_train)\n",
    "\n",
    "    # load nets into gpu\n",
    "    # if len(gpus) > 1:\n",
    "    #     segmentation_module = UserScatteredDataParallel(\n",
    "    #         segmentation_module,\n",
    "    #         device_ids=gpus)\n",
    "    #     # For sync bn\n",
    "    #     patch_replication_callback(segmentation_module)\n",
    "\n",
    "    # Set up optimizers\n",
    "    segmentation_module = createDeepLabv3(outputchannels=150)\n",
    "    optimizers = torch.optim.Adam(segmentation_module.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    segmentation_module.cuda()\n",
    "\n",
    "    # Main loop\n",
    "    history = {'train': {'epoch': [], 'loss': [], 'acc': []}}\n",
    "\n",
    "    for epoch in range(0, 1):\n",
    "        train(segmentation_module, iterator_train, optimizers, criterion, history, epoch+1)\n",
    "\n",
    "        # checkpointing\n",
    "        checkpoint(segmentation_module, history, epoch+1)\n",
    "\n",
    "    print('Training Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples: 20210\n",
      "1 Epoch = 5000 iters\n",
      "Epoch: [1][0/5000], Time: 0.38, Data: 0.00, Accuracy: 6.80, Loss: 4.704109\n",
      "Epoch: [1][20/5000], Time: 0.62, Data: 0.00, Accuracy: 21.07, Loss: 3.965649\n",
      "Epoch: [1][40/5000], Time: 0.58, Data: 0.00, Accuracy: 21.47, Loss: 3.643099\n",
      "Epoch: [1][60/5000], Time: 0.55, Data: 0.00, Accuracy: 23.61, Loss: 3.444899\n",
      "Epoch: [1][80/5000], Time: 0.54, Data: 0.00, Accuracy: 23.47, Loss: 3.403451\n",
      "Epoch: [1][100/5000], Time: 0.54, Data: 0.00, Accuracy: 24.38, Loss: 3.383046\n",
      "Epoch: [1][120/5000], Time: 0.54, Data: 0.00, Accuracy: 24.97, Loss: 3.322392\n",
      "Epoch: [1][140/5000], Time: 0.54, Data: 0.00, Accuracy: 25.66, Loss: 3.266722\n",
      "Epoch: [1][160/5000], Time: 0.54, Data: 0.00, Accuracy: 25.54, Loss: 3.211850\n",
      "Epoch: [1][180/5000], Time: 0.55, Data: 0.00, Accuracy: 25.75, Loss: 3.192372\n",
      "Epoch: [1][200/5000], Time: 0.55, Data: 0.00, Accuracy: 26.44, Loss: 3.167442\n",
      "Epoch: [1][220/5000], Time: 0.55, Data: 0.00, Accuracy: 26.65, Loss: 3.134791\n",
      "Epoch: [1][240/5000], Time: 0.55, Data: 0.00, Accuracy: 27.08, Loss: 3.116453\n",
      "Epoch: [1][260/5000], Time: 0.55, Data: 0.00, Accuracy: 27.40, Loss: 3.092560\n",
      "Epoch: [1][280/5000], Time: 0.54, Data: 0.00, Accuracy: 27.68, Loss: 3.081482\n",
      "Epoch: [1][300/5000], Time: 0.55, Data: 0.00, Accuracy: 28.00, Loss: 3.061491\n",
      "Epoch: [1][320/5000], Time: 0.55, Data: 0.00, Accuracy: 28.19, Loss: 3.050150\n",
      "Epoch: [1][340/5000], Time: 0.55, Data: 0.00, Accuracy: 28.53, Loss: 3.026843\n",
      "Epoch: [1][360/5000], Time: 0.55, Data: 0.00, Accuracy: 28.80, Loss: 3.009310\n",
      "Epoch: [1][380/5000], Time: 0.55, Data: 0.00, Accuracy: 28.94, Loss: 3.001187\n",
      "Epoch: [1][400/5000], Time: 0.55, Data: 0.00, Accuracy: 29.14, Loss: 2.992808\n",
      "Epoch: [1][420/5000], Time: 0.55, Data: 0.00, Accuracy: 29.53, Loss: 2.980053\n",
      "Epoch: [1][440/5000], Time: 0.55, Data: 0.00, Accuracy: 29.72, Loss: 2.973889\n",
      "Epoch: [1][460/5000], Time: 0.55, Data: 0.00, Accuracy: 29.98, Loss: 2.965928\n",
      "Epoch: [1][480/5000], Time: 0.55, Data: 0.00, Accuracy: 30.23, Loss: 2.949646\n",
      "Epoch: [1][500/5000], Time: 0.56, Data: 0.00, Accuracy: 30.40, Loss: 2.938650\n",
      "Epoch: [1][520/5000], Time: 0.56, Data: 0.00, Accuracy: 30.55, Loss: 2.931827\n",
      "Epoch: [1][540/5000], Time: 0.56, Data: 0.00, Accuracy: 30.70, Loss: 2.927496\n",
      "Epoch: [1][560/5000], Time: 0.56, Data: 0.00, Accuracy: 30.74, Loss: 2.926363\n",
      "Epoch: [1][580/5000], Time: 0.56, Data: 0.00, Accuracy: 30.93, Loss: 2.917585\n",
      "Epoch: [1][600/5000], Time: 0.56, Data: 0.00, Accuracy: 30.87, Loss: 2.912414\n",
      "Epoch: [1][620/5000], Time: 0.56, Data: 0.00, Accuracy: 30.85, Loss: 2.911306\n",
      "Epoch: [1][640/5000], Time: 0.56, Data: 0.00, Accuracy: 30.88, Loss: 2.906324\n",
      "Epoch: [1][660/5000], Time: 0.57, Data: 0.00, Accuracy: 31.18, Loss: 2.894082\n",
      "Epoch: [1][680/5000], Time: 0.57, Data: 0.00, Accuracy: 31.34, Loss: 2.886059\n",
      "Epoch: [1][700/5000], Time: 0.57, Data: 0.00, Accuracy: 31.35, Loss: 2.886235\n",
      "Epoch: [1][720/5000], Time: 0.56, Data: 0.00, Accuracy: 31.40, Loss: 2.888131\n",
      "Epoch: [1][740/5000], Time: 0.56, Data: 0.00, Accuracy: 31.46, Loss: 2.886075\n",
      "Epoch: [1][760/5000], Time: 0.57, Data: 0.00, Accuracy: 31.53, Loss: 2.875752\n",
      "Epoch: [1][780/5000], Time: 0.57, Data: 0.00, Accuracy: 31.63, Loss: 2.873571\n",
      "Epoch: [1][800/5000], Time: 0.57, Data: 0.00, Accuracy: 31.67, Loss: 2.870660\n",
      "Epoch: [1][820/5000], Time: 0.57, Data: 0.00, Accuracy: 31.74, Loss: 2.868215\n",
      "Epoch: [1][840/5000], Time: 0.57, Data: 0.00, Accuracy: 31.76, Loss: 2.864660\n",
      "Epoch: [1][860/5000], Time: 0.56, Data: 0.00, Accuracy: 31.86, Loss: 2.863494\n",
      "Epoch: [1][880/5000], Time: 0.56, Data: 0.00, Accuracy: 31.82, Loss: 2.863039\n",
      "Epoch: [1][900/5000], Time: 0.56, Data: 0.00, Accuracy: 31.78, Loss: 2.866583\n",
      "Epoch: [1][920/5000], Time: 0.56, Data: 0.00, Accuracy: 31.91, Loss: 2.859850\n",
      "Epoch: [1][940/5000], Time: 0.56, Data: 0.00, Accuracy: 32.03, Loss: 2.854540\n",
      "Epoch: [1][960/5000], Time: 0.57, Data: 0.00, Accuracy: 32.18, Loss: 2.850010\n",
      "Epoch: [1][980/5000], Time: 0.56, Data: 0.00, Accuracy: 32.25, Loss: 2.847283\n",
      "Epoch: [1][1000/5000], Time: 0.56, Data: 0.00, Accuracy: 32.33, Loss: 2.847540\n",
      "Epoch: [1][1020/5000], Time: 0.56, Data: 0.00, Accuracy: 32.35, Loss: 2.846191\n",
      "Epoch: [1][1040/5000], Time: 0.56, Data: 0.00, Accuracy: 32.53, Loss: 2.840777\n",
      "Epoch: [1][1060/5000], Time: 0.56, Data: 0.00, Accuracy: 32.65, Loss: 2.832635\n",
      "Epoch: [1][1080/5000], Time: 0.56, Data: 0.00, Accuracy: 32.77, Loss: 2.830124\n",
      "Epoch: [1][1100/5000], Time: 0.56, Data: 0.00, Accuracy: 32.84, Loss: 2.827305\n",
      "Epoch: [1][1120/5000], Time: 0.56, Data: 0.00, Accuracy: 32.81, Loss: 2.826940\n",
      "Epoch: [1][1140/5000], Time: 0.56, Data: 0.00, Accuracy: 32.86, Loss: 2.822370\n",
      "Epoch: [1][1160/5000], Time: 0.57, Data: 0.00, Accuracy: 32.88, Loss: 2.821499\n",
      "Epoch: [1][1180/5000], Time: 0.56, Data: 0.00, Accuracy: 32.84, Loss: 2.819133\n",
      "Epoch: [1][1200/5000], Time: 0.56, Data: 0.00, Accuracy: 32.92, Loss: 2.816370\n",
      "Epoch: [1][1220/5000], Time: 0.56, Data: 0.00, Accuracy: 33.01, Loss: 2.814769\n",
      "Epoch: [1][1240/5000], Time: 0.57, Data: 0.00, Accuracy: 33.05, Loss: 2.813549\n",
      "Epoch: [1][1260/5000], Time: 0.57, Data: 0.00, Accuracy: 33.21, Loss: 2.808019\n",
      "Epoch: [1][1280/5000], Time: 0.57, Data: 0.00, Accuracy: 33.19, Loss: 2.806430\n",
      "Epoch: [1][1300/5000], Time: 0.57, Data: 0.00, Accuracy: 33.23, Loss: 2.805405\n",
      "Epoch: [1][1320/5000], Time: 0.57, Data: 0.00, Accuracy: 33.33, Loss: 2.799261\n",
      "Epoch: [1][1340/5000], Time: 0.57, Data: 0.00, Accuracy: 33.34, Loss: 2.800457\n",
      "Epoch: [1][1360/5000], Time: 0.57, Data: 0.00, Accuracy: 33.44, Loss: 2.796127\n",
      "Epoch: [1][1380/5000], Time: 0.57, Data: 0.00, Accuracy: 33.50, Loss: 2.792959\n",
      "Epoch: [1][1400/5000], Time: 0.57, Data: 0.00, Accuracy: 33.47, Loss: 2.790385\n",
      "Epoch: [1][1420/5000], Time: 0.57, Data: 0.00, Accuracy: 33.55, Loss: 2.787435\n",
      "Epoch: [1][1440/5000], Time: 0.57, Data: 0.00, Accuracy: 33.67, Loss: 2.783043\n",
      "Epoch: [1][1460/5000], Time: 0.57, Data: 0.00, Accuracy: 33.74, Loss: 2.777886\n",
      "Epoch: [1][1480/5000], Time: 0.57, Data: 0.00, Accuracy: 33.72, Loss: 2.778135\n",
      "Epoch: [1][1500/5000], Time: 0.57, Data: 0.00, Accuracy: 33.80, Loss: 2.773571\n",
      "Epoch: [1][1520/5000], Time: 0.57, Data: 0.00, Accuracy: 33.85, Loss: 2.771783\n",
      "Epoch: [1][1540/5000], Time: 0.57, Data: 0.00, Accuracy: 33.89, Loss: 2.770809\n",
      "Epoch: [1][1560/5000], Time: 0.57, Data: 0.00, Accuracy: 33.88, Loss: 2.770332\n",
      "Epoch: [1][1580/5000], Time: 0.57, Data: 0.00, Accuracy: 33.93, Loss: 2.766671\n",
      "Epoch: [1][1600/5000], Time: 0.57, Data: 0.00, Accuracy: 33.97, Loss: 2.767991\n",
      "Epoch: [1][1620/5000], Time: 0.57, Data: 0.00, Accuracy: 33.98, Loss: 2.766256\n",
      "Epoch: [1][1640/5000], Time: 0.57, Data: 0.00, Accuracy: 33.99, Loss: 2.765904\n",
      "Epoch: [1][1660/5000], Time: 0.57, Data: 0.00, Accuracy: 34.07, Loss: 2.763372\n",
      "Epoch: [1][1680/5000], Time: 0.57, Data: 0.00, Accuracy: 34.10, Loss: 2.762939\n",
      "Epoch: [1][1700/5000], Time: 0.57, Data: 0.00, Accuracy: 34.11, Loss: 2.761595\n",
      "Epoch: [1][1720/5000], Time: 0.57, Data: 0.00, Accuracy: 34.19, Loss: 2.756917\n",
      "Epoch: [1][1740/5000], Time: 0.57, Data: 0.00, Accuracy: 34.24, Loss: 2.753903\n",
      "Epoch: [1][1760/5000], Time: 0.57, Data: 0.00, Accuracy: 34.30, Loss: 2.752135\n",
      "Epoch: [1][1780/5000], Time: 0.57, Data: 0.00, Accuracy: 34.32, Loss: 2.751523\n",
      "Epoch: [1][1800/5000], Time: 0.57, Data: 0.00, Accuracy: 34.36, Loss: 2.751613\n",
      "Epoch: [1][1820/5000], Time: 0.57, Data: 0.00, Accuracy: 34.35, Loss: 2.752162\n",
      "Epoch: [1][1840/5000], Time: 0.57, Data: 0.00, Accuracy: 34.34, Loss: 2.753831\n",
      "Epoch: [1][1860/5000], Time: 0.57, Data: 0.00, Accuracy: 34.39, Loss: 2.751380\n",
      "Epoch: [1][1880/5000], Time: 0.57, Data: 0.00, Accuracy: 34.48, Loss: 2.748006\n",
      "Epoch: [1][1900/5000], Time: 0.57, Data: 0.00, Accuracy: 34.52, Loss: 2.745946\n",
      "Epoch: [1][1920/5000], Time: 0.57, Data: 0.00, Accuracy: 34.56, Loss: 2.744016\n",
      "Epoch: [1][1940/5000], Time: 0.57, Data: 0.00, Accuracy: 34.58, Loss: 2.743787\n",
      "Epoch: [1][1960/5000], Time: 0.57, Data: 0.00, Accuracy: 34.55, Loss: 2.745300\n",
      "Epoch: [1][1980/5000], Time: 0.57, Data: 0.00, Accuracy: 34.65, Loss: 2.743468\n",
      "Epoch: [1][2000/5000], Time: 0.57, Data: 0.00, Accuracy: 34.69, Loss: 2.742263\n",
      "Epoch: [1][2020/5000], Time: 0.57, Data: 0.00, Accuracy: 34.69, Loss: 2.741814\n",
      "Epoch: [1][2040/5000], Time: 0.57, Data: 0.00, Accuracy: 34.74, Loss: 2.739816\n",
      "Epoch: [1][2060/5000], Time: 0.57, Data: 0.00, Accuracy: 34.75, Loss: 2.739490\n",
      "Epoch: [1][2080/5000], Time: 0.57, Data: 0.00, Accuracy: 34.77, Loss: 2.738926\n",
      "Epoch: [1][2100/5000], Time: 0.57, Data: 0.00, Accuracy: 34.84, Loss: 2.735339\n",
      "Epoch: [1][2120/5000], Time: 0.57, Data: 0.00, Accuracy: 34.89, Loss: 2.732309\n",
      "Epoch: [1][2140/5000], Time: 0.57, Data: 0.00, Accuracy: 34.94, Loss: 2.731117\n",
      "Epoch: [1][2160/5000], Time: 0.57, Data: 0.00, Accuracy: 34.94, Loss: 2.729930\n",
      "Epoch: [1][2180/5000], Time: 0.57, Data: 0.00, Accuracy: 34.98, Loss: 2.728676\n",
      "Epoch: [1][2200/5000], Time: 0.57, Data: 0.00, Accuracy: 35.03, Loss: 2.726376\n",
      "Epoch: [1][2220/5000], Time: 0.57, Data: 0.00, Accuracy: 35.07, Loss: 2.724766\n",
      "Epoch: [1][2240/5000], Time: 0.57, Data: 0.00, Accuracy: 35.04, Loss: 2.727321\n",
      "Epoch: [1][2260/5000], Time: 0.57, Data: 0.00, Accuracy: 35.03, Loss: 2.727590\n",
      "Epoch: [1][2280/5000], Time: 0.57, Data: 0.00, Accuracy: 35.06, Loss: 2.726070\n",
      "Epoch: [1][2300/5000], Time: 0.57, Data: 0.00, Accuracy: 35.07, Loss: 2.725484\n",
      "Epoch: [1][2320/5000], Time: 0.57, Data: 0.00, Accuracy: 35.11, Loss: 2.725731\n",
      "Epoch: [1][2340/5000], Time: 0.57, Data: 0.00, Accuracy: 35.11, Loss: 2.725266\n",
      "Epoch: [1][2360/5000], Time: 0.57, Data: 0.00, Accuracy: 35.13, Loss: 2.725645\n",
      "Epoch: [1][2380/5000], Time: 0.57, Data: 0.00, Accuracy: 35.20, Loss: 2.721935\n",
      "Epoch: [1][2400/5000], Time: 0.56, Data: 0.00, Accuracy: 35.23, Loss: 2.722507\n",
      "Epoch: [1][2420/5000], Time: 0.57, Data: 0.00, Accuracy: 35.26, Loss: 2.722280\n",
      "Epoch: [1][2440/5000], Time: 0.56, Data: 0.00, Accuracy: 35.25, Loss: 2.722875\n",
      "Epoch: [1][2460/5000], Time: 0.56, Data: 0.00, Accuracy: 35.26, Loss: 2.721907\n",
      "Epoch: [1][2480/5000], Time: 0.57, Data: 0.00, Accuracy: 35.30, Loss: 2.721472\n",
      "Epoch: [1][2500/5000], Time: 0.56, Data: 0.00, Accuracy: 35.35, Loss: 2.718425\n",
      "Epoch: [1][2520/5000], Time: 0.56, Data: 0.00, Accuracy: 35.39, Loss: 2.717138\n",
      "Epoch: [1][2540/5000], Time: 0.57, Data: 0.00, Accuracy: 35.42, Loss: 2.714995\n",
      "Epoch: [1][2560/5000], Time: 0.57, Data: 0.00, Accuracy: 35.45, Loss: 2.713631\n",
      "Epoch: [1][2580/5000], Time: 0.56, Data: 0.00, Accuracy: 35.46, Loss: 2.713415\n",
      "Epoch: [1][2600/5000], Time: 0.57, Data: 0.00, Accuracy: 35.48, Loss: 2.712903\n",
      "Epoch: [1][2620/5000], Time: 0.57, Data: 0.00, Accuracy: 35.52, Loss: 2.711019\n",
      "Epoch: [1][2640/5000], Time: 0.57, Data: 0.00, Accuracy: 35.54, Loss: 2.710016\n",
      "Epoch: [1][2660/5000], Time: 0.57, Data: 0.00, Accuracy: 35.56, Loss: 2.707810\n",
      "Epoch: [1][2680/5000], Time: 0.57, Data: 0.00, Accuracy: 35.55, Loss: 2.707173\n",
      "Epoch: [1][2700/5000], Time: 0.57, Data: 0.00, Accuracy: 35.57, Loss: 2.706735\n",
      "Epoch: [1][2720/5000], Time: 0.57, Data: 0.00, Accuracy: 35.58, Loss: 2.705094\n",
      "Epoch: [1][2740/5000], Time: 0.57, Data: 0.00, Accuracy: 35.59, Loss: 2.704927\n",
      "Epoch: [1][2760/5000], Time: 0.57, Data: 0.00, Accuracy: 35.64, Loss: 2.703342\n",
      "Epoch: [1][2780/5000], Time: 0.57, Data: 0.00, Accuracy: 35.67, Loss: 2.701894\n",
      "Epoch: [1][2800/5000], Time: 0.57, Data: 0.00, Accuracy: 35.62, Loss: 2.702979\n",
      "Epoch: [1][2820/5000], Time: 0.57, Data: 0.00, Accuracy: 35.72, Loss: 2.698625\n",
      "Epoch: [1][2840/5000], Time: 0.57, Data: 0.00, Accuracy: 35.74, Loss: 2.697542\n",
      "Epoch: [1][2860/5000], Time: 0.57, Data: 0.00, Accuracy: 35.79, Loss: 2.694732\n",
      "Epoch: [1][2880/5000], Time: 0.57, Data: 0.00, Accuracy: 35.84, Loss: 2.691547\n",
      "Epoch: [1][2900/5000], Time: 0.57, Data: 0.00, Accuracy: 35.88, Loss: 2.690413\n",
      "Epoch: [1][2920/5000], Time: 0.57, Data: 0.00, Accuracy: 35.91, Loss: 2.689724\n",
      "Epoch: [1][2940/5000], Time: 0.57, Data: 0.00, Accuracy: 35.94, Loss: 2.689251\n",
      "Epoch: [1][2960/5000], Time: 0.57, Data: 0.00, Accuracy: 35.97, Loss: 2.688275\n",
      "Epoch: [1][2980/5000], Time: 0.57, Data: 0.00, Accuracy: 36.01, Loss: 2.686159\n",
      "Epoch: [1][3000/5000], Time: 0.57, Data: 0.00, Accuracy: 36.04, Loss: 2.685282\n",
      "Epoch: [1][3020/5000], Time: 0.57, Data: 0.00, Accuracy: 36.06, Loss: 2.684381\n",
      "Epoch: [1][3040/5000], Time: 0.57, Data: 0.00, Accuracy: 36.08, Loss: 2.684636\n",
      "Epoch: [1][3060/5000], Time: 0.57, Data: 0.00, Accuracy: 36.08, Loss: 2.684431\n",
      "Epoch: [1][3080/5000], Time: 0.57, Data: 0.00, Accuracy: 36.12, Loss: 2.682936\n",
      "Epoch: [1][3100/5000], Time: 0.57, Data: 0.00, Accuracy: 36.19, Loss: 2.679321\n",
      "Epoch: [1][3120/5000], Time: 0.57, Data: 0.00, Accuracy: 36.20, Loss: 2.678881\n",
      "Epoch: [1][3140/5000], Time: 0.57, Data: 0.00, Accuracy: 36.24, Loss: 2.676829\n",
      "Epoch: [1][3160/5000], Time: 0.57, Data: 0.00, Accuracy: 36.25, Loss: 2.676670\n",
      "Epoch: [1][3180/5000], Time: 0.57, Data: 0.00, Accuracy: 36.30, Loss: 2.674708\n",
      "Epoch: [1][3200/5000], Time: 0.57, Data: 0.00, Accuracy: 36.31, Loss: 2.674017\n",
      "Epoch: [1][3220/5000], Time: 0.57, Data: 0.00, Accuracy: 36.34, Loss: 2.672566\n",
      "Epoch: [1][3240/5000], Time: 0.57, Data: 0.00, Accuracy: 36.35, Loss: 2.671531\n",
      "Epoch: [1][3260/5000], Time: 0.57, Data: 0.00, Accuracy: 36.38, Loss: 2.670843\n",
      "Epoch: [1][3280/5000], Time: 0.57, Data: 0.00, Accuracy: 36.39, Loss: 2.669216\n",
      "Epoch: [1][3300/5000], Time: 0.57, Data: 0.00, Accuracy: 36.40, Loss: 2.669586\n",
      "Epoch: [1][3320/5000], Time: 0.57, Data: 0.00, Accuracy: 36.41, Loss: 2.668946\n",
      "Epoch: [1][3340/5000], Time: 0.57, Data: 0.00, Accuracy: 36.42, Loss: 2.668498\n",
      "Epoch: [1][3360/5000], Time: 0.57, Data: 0.00, Accuracy: 36.44, Loss: 2.667906\n",
      "Epoch: [1][3380/5000], Time: 0.57, Data: 0.00, Accuracy: 36.46, Loss: 2.667068\n",
      "Epoch: [1][3400/5000], Time: 0.57, Data: 0.00, Accuracy: 36.50, Loss: 2.665331\n",
      "Epoch: [1][3420/5000], Time: 0.57, Data: 0.00, Accuracy: 36.52, Loss: 2.663890\n",
      "Epoch: [1][3440/5000], Time: 0.57, Data: 0.00, Accuracy: 36.55, Loss: 2.662615\n",
      "Epoch: [1][3460/5000], Time: 0.57, Data: 0.00, Accuracy: 36.58, Loss: 2.661787\n",
      "Epoch: [1][3480/5000], Time: 0.57, Data: 0.00, Accuracy: 36.64, Loss: 2.659181\n",
      "Epoch: [1][3500/5000], Time: 0.57, Data: 0.00, Accuracy: 36.66, Loss: 2.658626\n",
      "Epoch: [1][3520/5000], Time: 0.57, Data: 0.00, Accuracy: 36.69, Loss: 2.658077\n",
      "Epoch: [1][3540/5000], Time: 0.57, Data: 0.00, Accuracy: 36.70, Loss: 2.656887\n",
      "Epoch: [1][3560/5000], Time: 0.57, Data: 0.00, Accuracy: 36.75, Loss: 2.654711\n",
      "Epoch: [1][3580/5000], Time: 0.57, Data: 0.00, Accuracy: 36.79, Loss: 2.652470\n",
      "Epoch: [1][3600/5000], Time: 0.57, Data: 0.00, Accuracy: 36.79, Loss: 2.652408\n",
      "Epoch: [1][3620/5000], Time: 0.57, Data: 0.00, Accuracy: 36.84, Loss: 2.650280\n",
      "Epoch: [1][3640/5000], Time: 0.57, Data: 0.00, Accuracy: 36.87, Loss: 2.649146\n",
      "Epoch: [1][3660/5000], Time: 0.57, Data: 0.00, Accuracy: 36.88, Loss: 2.648665\n",
      "Epoch: [1][3680/5000], Time: 0.57, Data: 0.00, Accuracy: 36.91, Loss: 2.647378\n",
      "Epoch: [1][3700/5000], Time: 0.57, Data: 0.00, Accuracy: 36.95, Loss: 2.645872\n",
      "Epoch: [1][3720/5000], Time: 0.57, Data: 0.00, Accuracy: 36.97, Loss: 2.645399\n",
      "Epoch: [1][3740/5000], Time: 0.57, Data: 0.00, Accuracy: 37.03, Loss: 2.643391\n",
      "Epoch: [1][3760/5000], Time: 0.57, Data: 0.00, Accuracy: 37.05, Loss: 2.642091\n",
      "Epoch: [1][3780/5000], Time: 0.57, Data: 0.00, Accuracy: 37.09, Loss: 2.640763\n",
      "Epoch: [1][3800/5000], Time: 0.57, Data: 0.00, Accuracy: 37.10, Loss: 2.640733\n",
      "Epoch: [1][3820/5000], Time: 0.57, Data: 0.00, Accuracy: 37.11, Loss: 2.639919\n",
      "Epoch: [1][3840/5000], Time: 0.57, Data: 0.00, Accuracy: 37.14, Loss: 2.638370\n",
      "Epoch: [1][3860/5000], Time: 0.57, Data: 0.00, Accuracy: 37.19, Loss: 2.636550\n",
      "Epoch: [1][3880/5000], Time: 0.57, Data: 0.00, Accuracy: 37.20, Loss: 2.636974\n",
      "Epoch: [1][3900/5000], Time: 0.57, Data: 0.00, Accuracy: 37.21, Loss: 2.636773\n",
      "Epoch: [1][3920/5000], Time: 0.57, Data: 0.00, Accuracy: 37.25, Loss: 2.636015\n",
      "Epoch: [1][3940/5000], Time: 0.57, Data: 0.00, Accuracy: 37.30, Loss: 2.633289\n",
      "Epoch: [1][3960/5000], Time: 0.57, Data: 0.00, Accuracy: 37.34, Loss: 2.630949\n",
      "Epoch: [1][3980/5000], Time: 0.57, Data: 0.00, Accuracy: 37.38, Loss: 2.629363\n",
      "Epoch: [1][4000/5000], Time: 0.57, Data: 0.00, Accuracy: 37.39, Loss: 2.628774\n",
      "Epoch: [1][4020/5000], Time: 0.57, Data: 0.00, Accuracy: 37.42, Loss: 2.627449\n",
      "Epoch: [1][4040/5000], Time: 0.57, Data: 0.00, Accuracy: 37.39, Loss: 2.628531\n",
      "Epoch: [1][4060/5000], Time: 0.57, Data: 0.00, Accuracy: 37.41, Loss: 2.628211\n",
      "Epoch: [1][4080/5000], Time: 0.57, Data: 0.00, Accuracy: 37.41, Loss: 2.628450\n",
      "Epoch: [1][4100/5000], Time: 0.57, Data: 0.00, Accuracy: 37.45, Loss: 2.626998\n",
      "Epoch: [1][4120/5000], Time: 0.57, Data: 0.00, Accuracy: 37.47, Loss: 2.625670\n",
      "Epoch: [1][4140/5000], Time: 0.57, Data: 0.00, Accuracy: 37.50, Loss: 2.623853\n",
      "Epoch: [1][4160/5000], Time: 0.57, Data: 0.00, Accuracy: 37.53, Loss: 2.622273\n",
      "Epoch: [1][4180/5000], Time: 0.57, Data: 0.00, Accuracy: 37.56, Loss: 2.621038\n",
      "Epoch: [1][4200/5000], Time: 0.57, Data: 0.00, Accuracy: 37.59, Loss: 2.620298\n",
      "Epoch: [1][4220/5000], Time: 0.57, Data: 0.00, Accuracy: 37.60, Loss: 2.619696\n",
      "Epoch: [1][4240/5000], Time: 0.57, Data: 0.00, Accuracy: 37.60, Loss: 2.619903\n",
      "Epoch: [1][4260/5000], Time: 0.57, Data: 0.00, Accuracy: 37.60, Loss: 2.619779\n",
      "Epoch: [1][4280/5000], Time: 0.57, Data: 0.00, Accuracy: 37.63, Loss: 2.618839\n",
      "Epoch: [1][4300/5000], Time: 0.57, Data: 0.00, Accuracy: 37.66, Loss: 2.618429\n",
      "Epoch: [1][4320/5000], Time: 0.57, Data: 0.00, Accuracy: 37.68, Loss: 2.617301\n",
      "Epoch: [1][4340/5000], Time: 0.57, Data: 0.00, Accuracy: 37.67, Loss: 2.617081\n",
      "Epoch: [1][4360/5000], Time: 0.57, Data: 0.00, Accuracy: 37.70, Loss: 2.616224\n",
      "Epoch: [1][4380/5000], Time: 0.57, Data: 0.00, Accuracy: 37.71, Loss: 2.615547\n",
      "Epoch: [1][4400/5000], Time: 0.57, Data: 0.00, Accuracy: 37.72, Loss: 2.614728\n",
      "Epoch: [1][4420/5000], Time: 0.57, Data: 0.00, Accuracy: 37.72, Loss: 2.614635\n",
      "Epoch: [1][4440/5000], Time: 0.57, Data: 0.00, Accuracy: 37.74, Loss: 2.613892\n",
      "Epoch: [1][4460/5000], Time: 0.57, Data: 0.00, Accuracy: 37.76, Loss: 2.613904\n",
      "Epoch: [1][4480/5000], Time: 0.57, Data: 0.00, Accuracy: 37.77, Loss: 2.613438\n",
      "Epoch: [1][4500/5000], Time: 0.57, Data: 0.00, Accuracy: 37.79, Loss: 2.612235\n",
      "Epoch: [1][4520/5000], Time: 0.57, Data: 0.00, Accuracy: 37.83, Loss: 2.610326\n",
      "Epoch: [1][4540/5000], Time: 0.57, Data: 0.00, Accuracy: 37.85, Loss: 2.609228\n",
      "Epoch: [1][4560/5000], Time: 0.57, Data: 0.00, Accuracy: 37.85, Loss: 2.609289\n",
      "Epoch: [1][4580/5000], Time: 0.57, Data: 0.00, Accuracy: 37.87, Loss: 2.608004\n",
      "Epoch: [1][4600/5000], Time: 0.57, Data: 0.00, Accuracy: 37.90, Loss: 2.606403\n",
      "Epoch: [1][4620/5000], Time: 0.57, Data: 0.00, Accuracy: 37.91, Loss: 2.605823\n",
      "Epoch: [1][4640/5000], Time: 0.57, Data: 0.00, Accuracy: 37.96, Loss: 2.604258\n",
      "Epoch: [1][4660/5000], Time: 0.57, Data: 0.00, Accuracy: 37.96, Loss: 2.605006\n",
      "Epoch: [1][4680/5000], Time: 0.57, Data: 0.00, Accuracy: 37.97, Loss: 2.605205\n",
      "Epoch: [1][4700/5000], Time: 0.57, Data: 0.00, Accuracy: 38.01, Loss: 2.603510\n",
      "Epoch: [1][4720/5000], Time: 0.57, Data: 0.00, Accuracy: 38.01, Loss: 2.603150\n",
      "Epoch: [1][4740/5000], Time: 0.57, Data: 0.00, Accuracy: 38.04, Loss: 2.601505\n",
      "Epoch: [1][4760/5000], Time: 0.57, Data: 0.00, Accuracy: 38.04, Loss: 2.601552\n",
      "Epoch: [1][4780/5000], Time: 0.57, Data: 0.00, Accuracy: 38.06, Loss: 2.600547\n",
      "Epoch: [1][4800/5000], Time: 0.57, Data: 0.00, Accuracy: 38.09, Loss: 2.598778\n",
      "Epoch: [1][4820/5000], Time: 0.57, Data: 0.00, Accuracy: 38.12, Loss: 2.597734\n",
      "Epoch: [1][4840/5000], Time: 0.57, Data: 0.00, Accuracy: 38.13, Loss: 2.597636\n",
      "Epoch: [1][4860/5000], Time: 0.57, Data: 0.00, Accuracy: 38.16, Loss: 2.596508\n",
      "Epoch: [1][4880/5000], Time: 0.57, Data: 0.00, Accuracy: 38.16, Loss: 2.596507\n",
      "Epoch: [1][4900/5000], Time: 0.57, Data: 0.00, Accuracy: 38.17, Loss: 2.596144\n",
      "Epoch: [1][4920/5000], Time: 0.57, Data: 0.00, Accuracy: 38.22, Loss: 2.593906\n",
      "Epoch: [1][4940/5000], Time: 0.57, Data: 0.00, Accuracy: 38.22, Loss: 2.593414\n",
      "Epoch: [1][4960/5000], Time: 0.57, Data: 0.00, Accuracy: 38.22, Loss: 2.593558\n",
      "Epoch: [1][4980/5000], Time: 0.57, Data: 0.00, Accuracy: 38.23, Loss: 2.593471\n",
      "Saving checkpoints...\n",
      "Training Done!\n"
     ]
    }
   ],
   "source": [
    "# writer = SummaryWriter('runs/ade20k-resnet50dilated-ppm_deepsup_experiment_1')\n",
    "DIR = \"./ckpt/ade20k-resnet50dilated-ppm_deepsup\"\n",
    "if not os.path.isdir(DIR):\n",
    "    os.makedirs(DIR)\n",
    "# parser = argparse.ArgumentParser(\n",
    "#     description=\"PyTorch Semantic Segmentation Training\"\n",
    "# )\n",
    "\n",
    "# parser.add_argument(\n",
    "#     \"--gpus\",\n",
    "#     default=\"0-3\",\n",
    "#     help=\"gpus to use, e.g. 0-3 or 0,1,2,3\"\n",
    "# )\n",
    "\n",
    "\n",
    "random.seed(869)\n",
    "torch.manual_seed(869)\n",
    "writer = SummaryWriter('runs/ade20k-resnet50dilated-ppm_deepsup_experiment_1')\n",
    "main()\n",
    "\n",
    "# Start from checkpoint\n",
    "# start_epoch = 0\n",
    "# if start_epoch > 0:\n",
    "#     cfg.MODEL.weights_encoder = os.path.join(\n",
    "#         cfg.DIR, 'encoder_epoch_{}.pth'.format(start_epoch))\n",
    "#     cfg.MODEL.weights_decoder = os.path.join(\n",
    "#         cfg.DIR, 'decoder_epoch_{}.pth'.format(start_epoch))\n",
    "#     assert os.path.exists(cfg.MODEL.weights_encoder) and \\\n",
    "#         os.path.exists(cfg.MODEL.weights_decoder), \"checkpoint does not exitst!\"\n",
    "\n",
    "# # Parse gpu ids\n",
    "# gpus = parse_devices(args.gpus)\n",
    "# gpus = [x.replace('gpu', '') for x in gpus]\n",
    "# gpus = [int(x) for x in gpus]\n",
    "# num_gpus = len(gpus)\n",
    "# batch_size_per_gpu = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1651003465.8851838"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic = time.time()\n",
    "tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.857238531112671"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic2 = time.time()\n",
    "tic2-tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###############################################################\n",
      "\n",
      "       _____                            _____            \n",
      "      / ____|                          / ____|           \n",
      "     | (___   ___ ___ _ __   ___ _____| (___   ___  __ _ \n",
      "      \\___ \\ / __/ _ \\ '_ \\ / _ \\______\\___ \\ / _ \\/ _` |\n",
      "      ____) | (_|  __/ | | |  __/      ____) |  __/ (_| |\n",
      "     |_____/ \\___\\___|_| |_|\\___|     |_____/ \\___|\\__, |\n",
      "                                                    __/ |\n",
      "                                                   |___/                                                                    \n",
      "\n",
      "###############################################################\n",
      "Welcome to use Scene-Seg (S2)! This is a PyTorch implementation \n",
      "of semantic segmentation models on MIT ADE20K scene parsing \n",
      "dataset (http://sceneparsing.csail.mit.edu/). \n",
      "\n",
      "Developing this tool is also the main part of MIT 6.869 project.\n",
      "\n",
      "-- Author: Zijie Zhao\n",
      "-- Date: Apr 26 2022\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "###############################################################\n",
    "\n",
    "       _____                            _____            \n",
    "      / ____|                          / ____|           \n",
    "     | (___   ___ ___ _ __   ___ _____| (___   ___  __ _ \n",
    "      \\___ \\ / __/ _ \\ '_ \\ / _ \\______\\___ \\ / _ \\/ _` |\n",
    "      ____) | (_|  __/ | | |  __/      ____) |  __/ (_| |\n",
    "     |_____/ \\___\\___|_| |_|\\___|     |_____/ \\___|\\__, |\n",
    "                                                    __/ |\n",
    "                                                   |___/                                                                    \n",
    "\n",
    "###############################################################\n",
    "Welcome to use Scene-Seg (S2)! This is a PyTorch implementation \n",
    "of semantic segmentation models on MIT ADE20K scene parsing \n",
    "dataset (http://sceneparsing.csail.mit.edu/). \n",
    "\n",
    "Developing this tool is also the main part of MIT 6.869 project.\n",
    "\n",
    "-- Author: Zijie Zhao\n",
    "-- Date: Apr 26 2022\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "a = 1\n",
    "def foo():\n",
    "    global a \n",
    "    a = a +1\n",
    "    print(a)\n",
    "foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 PowerAI Base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
